{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master HN PSL ‚Äì NLP - 29/03/2022\n",
    "\n",
    "## [Spacy](https://spacy.io)\n",
    "\n",
    "Notebook con√ßu par C. Plancq (2021), mise √† jour T. Poibeau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Biblioth√®que logicielle de TAL √©crite en Python (et Cython)\n",
    "- √âtiquetage POS, lemmatisation, analyse syntaxique, entit√©s nomm√©es, word embedding, transformers\n",
    "- Usage de mod√®les neuronaux\n",
    "- Int√©gration ais√©e de biblioth√®ques de deep learning\n",
    "- v3.0.3 ([github](https://github.com/explosion/spaCy))\n",
    "- Licence MIT (Open Source) pour le code\n",
    "    - Licences ouvertes diverses pour les mod√®les\n",
    "- Produit de la soci√©t√© [explosion.ai](https://explosion.ai/). Fond√© par :¬†Matthew Honnibal ([@honnibal](https://twitter.com/honnibal)) et Ines Montani ([@_inesmontani](https://twitter.com/_inesmontani))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pourquoi Spacy ?\n",
    "\n",
    "- C'est du Python üôå üéâ\n",
    "- Plut√¥t simple √† prendre en main\n",
    "- Tr√®s bien document√©, √† notre avis. D'ailleurs plut√¥t que ce notebook, suivez l'excellent tutorial d'Ines Montani : [https://course.spacy.io/](https://course.spacy.io/)\n",
    "- Couvre les traitements d'une cha√Æne de TAL typique\n",
    "- Pas mal utilis√© dans l'industrie\n",
    "- MAIS ce n'est pas forc√©ment l'outil qui donne les meilleurs r√©sultats pour le fran√ßais dans toutes les t√¢ches de TAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy et les autres\n",
    "\n",
    "Spacy est *un* des frameworks de TAL disponibles\n",
    "\n",
    "- [NLTK](http://www.nltk.org/) :¬†python, orient√© p√©dagogie, pas de mod√®les neuronaux inclus mais se combine bien avec TensorFlow, PyTorch ou AlleNLP\n",
    "- [Stanford Core¬†NLP](https://stanfordnlp.github.io/stanfordnlp/) :¬†java, mod√®les pour 53 langues (UD), r√©solution de la cor√©ference.\n",
    "- [Stanza](https://stanfordnlp.github.io/stanza/) :¬†python, nouveau framework de Stanford, mod√®les neuronaux entra√Æn√©s sur donn√©es UD <small>[https://github.com/explosion/spacy-stanza](https://github.com/explosion/spacy-stanza) permet d'utiliser les mod√®les de Stanford avec Spacy</small>\n",
    "- [TextBlob](https://textblob.readthedocs.io/en/dev/)\n",
    "- [DKPro](https://dkpro.github.io/)\n",
    "- [flair](https://github.com/zalandoresearch/flair) : le framework de Zalando, tr√®s bonnes performances en reconnaissance d'entit√©s nomm√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## installation\n",
    "\n",
    "dans un terminal\n",
    "```bash\n",
    "python3 -m pip install -U --user spacy \n",
    "#ou pip install -U --user spacy\n",
    "```\n",
    "- installation du mod√®le fran√ßais\n",
    "```bash\n",
    "python3 -m spacy download fr_core_news_md\n",
    "#ou python3 -m spacy download fr_core_news_sm \n",
    "```\n",
    "- v√©rification\n",
    "```bash\n",
    "python3 -m spacy validate\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mod√®les\n",
    "\n",
    "- Spacy utilise des mod√®les statistiques qui permettent de pr√©dire des annotations linguistiques\n",
    "- 16 langues :¬†allemand, anglais, chinois, danois, espagnol, fran√ßais, italien, japonais, lituanien, n√©erlandais, grec, norv√©gien, polonais, portugais, roumain, russe + mod√®le multi langues\n",
    "- 4 mod√®les pour le fran√ßais\n",
    "    - fr_core_news_sm (tagger, morphologizer, lemmatizer, parser, ner) 16 Mo\n",
    "    - fr_core_news_md (tagger, morphologizer, lemmatizer, parser, ner, vectors) 45 Mo\n",
    "    - fr_core_news_lg (tagger, morphologizer, lemmatizer, parser, ner, vectors) 546 Mo\n",
    "    - fr_dep_news_trf (tagger, morphologizer, lemmatizer, parser) 381 Mo\n",
    "- mod√®les `fr` appris sur les corpus [Sequoia](https://deep-sequoia.inria.fr/fr/) et [WikiNer](https://figshare.com/articles/Learning_multilingual_named_entity_recognition_from_Wikipedia/5462500) sauf le mod√®le `trf` qui est issu de camembert-base distribu√© par [Hugging Face](https://huggingface.co/camembert-base).\n",
    "- Tous ces mod√®les, quelque soient leur type ou leur langue, s'utilisent de la m√™me fa√ßon, avec la m√™me API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage\n",
    "\n",
    "- *si vous voulez utiliser Spacy prenez le temps de lire la [documentation](https://spacy.io/usage), ici ce ne sera qu'un coup d'≈ìil incomplet*\n",
    "- un mod√®le est une instance de la classe `Language`, il est adapt√© √† une langue en particulier\n",
    "- un mod√®le incorpore un vocabulaire, des poids, des vecteurs de mots, une configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.fr.French"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- le traitement fonctionne avec un [*pipeline*](https://spacy.io/usage/spacy-101#pipelines) pour convertir un texte en objet `Doc` (texte annot√©)\n",
    "- par d√©faut `tokenizer` > `tagger` > `parser` > `ner` > `‚Ä¶`\n",
    "- depuis la v3 le pipeline devient `tok2vec` > `morphologizer` > `parser` > `ner` > `attribute_ruler` > `lemmatizer`  \n",
    "  ou `transformer` > `morphologizer` > `parser` > `ner` > `attribute_ruler` > `lemmatizer`\n",
    "- l'utilisateur peut ajouter des √©tapes ou en retrancher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x11fe9ef40>),\n",
       " ('morphologizer',\n",
       "  <spacy.pipeline.morphologizer.Morphologizer at 0x11fe9e280>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x12af0f780>),\n",
       " ('lemmatizer', <spacy.lang.fr.lemmatizer.FrenchLemmatizer at 0x288aae0c0>)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('fr_core_news_md', disable=[\"parser\", \"ner\"])\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retour au pipeline par d√©faut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x11f8c58e0>),\n",
       " ('morphologizer',\n",
       "  <spacy.pipeline.morphologizer.Morphologizer at 0x13a04aa60>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x11f902c10>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x11fcf6580>),\n",
       " ('lemmatizer', <spacy.lang.fr.lemmatizer.FrenchLemmatizer at 0x12a890680>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x11f902e40>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('fr_core_news_md')\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Un objet `Doc` est une s√©quence d'objets `Token` (voir l'[API](https://spacy.io/api/token))\n",
    " - Le texte d'origine est d√©coup√© en phrases, tokeniz√©, annot√© en POS, lemme, syntaxe (d√©pendance) et en entit√©s nomm√©es (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"L‚ÄôOrganisation des Nations unies (ONU) a lanc√© mardi un appel d‚Äôurgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus.\")\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì tokenization\n",
    "\n",
    "La tokenization de Spacy est non-destructive. Vous pouvez d√©couper un texte en tokens et le restituer dans sa forme originale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'\n",
      "Organisation\n",
      "des\n",
      "Nations\n",
      "unies\n",
      "(\n",
      "ONU\n",
      ")\n",
      "a\n",
      "lanc√©\n",
      "mardi\n",
      "un\n",
      "appel\n",
      "d'\n",
      "urgence\n",
      "pour\n",
      "lever\n",
      "des\n",
      "dizaines\n",
      "de\n",
      "millions\n",
      "de\n",
      "dollars\n",
      "afin\n",
      "de\n",
      "prot√©ger\n",
      "les\n",
      "r√©fugi√©s\n",
      "vuln√©rables\n",
      "face\n",
      "√†\n",
      "la\n",
      "propagation\n",
      "du\n",
      "nouveau\n",
      "coronavirus\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"L'Organisation des Nations unies (ONU) a lanc√© mardi un appel d'urgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus.\")\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'Organisation des Nations unies (ONU) a lanc√© mardi un appel d'urgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus."
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text_with_ws, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì √©tiquetage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les annotations portant sur les tokens sont accessibles via les attributs des objets de type `token`‚ÄØ: [https://spacy.io/api/token#attributes](https://spacy.io/api/token#attributes)  \n",
    "  - `pos_` contient l'√©tiquette de partie du discours de [universal dependancies](https://universaldependencies.org/docs/u/pos/)\n",
    "  - `tag_` contient l'√©tiquette du corpus original, parfois plus d√©taill√©e\n",
    "  - `lemma_` pour le lemme\n",
    "  - `morph` pour l'analyse morphologique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L' DET Definite=Def|Number=Sing|PronType=Art le\n",
      "Organisation NOUN Gender=Fem|Number=Sing organisation\n",
      "des ADP Definite=Def|Number=Plur|PronType=Art de\n",
      "Nations PROPN  Nations\n",
      "unies ADJ Gender=Fem|Number=Plur uni\n",
      "( PUNCT  (\n",
      "ONU PROPN Gender=Fem|Number=Sing ONU\n",
      ") PUNCT  )\n",
      "a AUX Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin avoir\n",
      "lanc√© VERB Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part lancer\n",
      "mardi NOUN Gender=Masc|Number=Sing mardi\n",
      "un DET Definite=Ind|Gender=Masc|Number=Sing|PronType=Art un\n",
      "appel NOUN Gender=Masc|Number=Sing appel\n",
      "d' ADP  de\n",
      "urgence NOUN Gender=Fem|Number=Sing urgence\n",
      "pour ADP  pour\n",
      "lever VERB VerbForm=Inf lever\n",
      "des DET Definite=Ind|Number=Plur|PronType=Art un\n",
      "dizaines NOUN Gender=Fem|Number=Plur dizaine\n",
      "de ADP  de\n",
      "millions NOUN Gender=Masc|NumType=Card|Number=Plur million\n",
      "de ADP  de\n",
      "dollars NOUN Gender=Masc|Number=Plur dollar\n",
      "afin ADV  afin\n",
      "de ADP  de\n",
      "prot√©ger VERB VerbForm=Inf prot√©ger\n",
      "les DET Definite=Def|Number=Plur|PronType=Art le\n",
      "r√©fugi√©s NOUN Gender=Masc|Number=Plur r√©fugi√©\n",
      "vuln√©rables ADJ Number=Plur vuln√©rable\n",
      "face NOUN Gender=Fem|Number=Sing face\n",
      "√† ADP  √†\n",
      "la DET Definite=Def|Gender=Fem|Number=Sing|PronType=Art le\n",
      "propagation NOUN Gender=Fem|Number=Sing propagation\n",
      "du ADP Definite=Def|Gender=Masc|Number=Sing|PronType=Art de\n",
      "nouveau ADJ Gender=Masc|Number=Sing nouveau\n",
      "coronavirus NOUN Gender=Masc|Number=Sing coronavirus\n",
      ". PUNCT  .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.morph, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour traiter plusieurs textes en s√©rie, il est recommand√© d'utiliser [nlp.pipe](https://spacy.io/api/language#pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Cadine avait un tr√®s-mauvais caract√®re. Elle ne s‚Äôaccommodait pas du r√¥le de servante.\",\n",
    "    \"Aussi finit-elle par s‚Äô√©tablir pour son compte.\",\n",
    "    \"Comme elle √©tait alors √¢g√©e de treize ans, et qu‚Äôelle ne pouvait r√™ver le grand commerce, un banc de vente de l‚Äôall√©e aux fleurs, elle vendit des bouquets de violettes d‚Äôun sou, piqu√©s dans un lit de mousse, sur un √©ventaire d‚Äôosier pendu √† son cou.\",\n",
    "    \"Elle r√¥dait toute la journ√©e dans les Halles, autour des Halles, promenant son bout de pelouse.\",\n",
    "    \"C‚Äô√©tait l√† sa joie, cette fl√¢nerie continuelle, qui lui d√©gourdissait les jambes, qui la tirait des longues heures pass√©es √† faire des bouquets, les genoux pli√©s, sur une chaise basse.\",\n",
    "    \"Maintenant, elle tournait ses violettes en marchant, elle les tournait comme des fuseaux, avec une merveilleuse l√©g√®ret√© de doigts ; elle comptait six √† huit fleurs, selon la saison, pliait en deux un brin de jonc, ajoutait une feuille, roulait un fil mouill√© ; et, entre ses dents de jeune loup, elle cassait le fil.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä¬†vous  \n",
    "1. Extrayez de la s√©rie de phrases ci-dessus la liste des noms communs\n",
    "2. Comptez le nombre de tokens au masculin et au f√©minin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sou, all√©e, mousse, osier, journ√©e, Halles, dents, compte, fleurs, lit, cou, caract√®re, l√©g√®ret√©, loup, brin, bout, vente, merveilleuse, √©ventaire, jambes, genoux, joie, bouquets, servante, commerce, feuille, fl√¢nerie, fil, doigts, banc, r√¥le, violettes, pelouse, heures, chaise, saison, ans, fuseaux\n"
     ]
    }
   ],
   "source": [
    "# Extrayez de la s√©rie de phrases ci-dessus la liste des noms communs\n",
    "\n",
    "ncs = []\n",
    "docs = nlp.pipe(texts)\n",
    "for doc in docs:\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == \"NOUN\":\n",
    "            ncs.append(tok.text)\n",
    "ncs = set(ncs)        \n",
    "print(\", \".join(ncs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Bonjour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(doc[0].morph.to_dict()[\"Gender\"]))\n",
    "print(type(doc[0].morph.get(\"Gender\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens au masculin :¬†38, tokens au f√©minin : 46\n"
     ]
    }
   ],
   "source": [
    "# Comptez le nombre de tokens au masculin et au f√©minin\n",
    "\n",
    "nb_masc = 0\n",
    "nb_fem = 0\n",
    "docs = nlp.pipe(texts)\n",
    "for doc in docs:\n",
    "    for tok in doc:\n",
    "        if tok.morph.get(\"Gender\") == [\"Masc\"]:\n",
    "            nb_masc += 1\n",
    "        elif tok.morph.get(\"Gender\") == [\"Fem\"]:\n",
    "            nb_fem += 1\n",
    "        \n",
    "print(f\"tokens au masculin :¬†{nb_masc}, tokens au f√©minin : {nb_fem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì analyse syntaxique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'analyse syntaxique ou *parsing* de Spacy est une analyse en d√©pendance. La plupart sinon la totalit√© des mod√®les utilis√©s viennent de https://universaldependencies.org\n",
    "\n",
    "Dans l'analyse en d√©pendance produite par Spacy, chaque mot d'une phrase a un gouverneur unique (*head*), la relation de d√©pendance entre le mot et son gouverneur est typ√©e (*nsubj*, *obj*, ‚Ä¶).  \n",
    "Pour la t√™te de la phrase on utilise la relation *ROOT*.\n",
    "\n",
    "La structure produite par l'analyse syntaxique est un arbre, un graphe acyclique et connexe. Les tokens sont les n≈ìuds, les arcs sont les d√©pendances, le type de la relation est l'√©tiquette de l'arc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`displacy` fournit un outil de visualisation bien pratique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'displacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDerri√®re lui, sur le carreau de la rue Rambuteau, on vendait des fruits.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdisplacy\u001b[49m\u001b[38;5;241m.\u001b[39mrender(doc, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdep\u001b[39m\u001b[38;5;124m\"\u001b[39m, jupyter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m90\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'displacy' is not defined"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Derri√®re lui, sur le carreau de la rue Rambuteau, on vendait des fruits.\")\n",
    "displacy.render(doc, style=\"dep\", jupyter=True, options={'distance':90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe √©galement un outil issu d'un d√©veloppement ind√©pendant :¬†[explacy](https://spacy.io/universe/project/explacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dep tree     Token     Dep type Lemma     Part of Sp\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "         ‚îå‚îÄ‚ñ∫ Derri√®re  case     derri√®re  ADP       \n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îî‚îÄ‚îÄ lui       obl:mod  lui       PRON      \n",
      "‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ,         punct    ,         PUNCT     \n",
      "‚îÇ‚îÇ      ‚îå‚îÄ‚îÄ‚ñ∫ sur       case     sur       ADP       \n",
      "‚îÇ‚îÇ      ‚îÇ‚îå‚îÄ‚ñ∫ le        det      le        DET       \n",
      "‚îÇ‚îÇ‚îå‚îÄ‚ñ∫‚îå‚îÄ‚îÄ‚î¥‚î¥‚îÄ‚îÄ carreau   obl:mod  carreau   NOUN      \n",
      "‚îÇ‚îÇ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚ñ∫ de        case     de        ADP       \n",
      "‚îÇ‚îÇ‚îÇ  ‚îÇ  ‚îÇ‚îå‚îÄ‚ñ∫ la        det      le        DET       \n",
      "‚îÇ‚îÇ‚îÇ  ‚îî‚îÄ‚ñ∫‚îî‚îº‚îÄ‚îÄ rue       nmod     rue       NOUN      \n",
      "‚îÇ‚îÇ‚îÇ      ‚îî‚îÄ‚ñ∫ Rambuteau nmod     Rambuteau PROPN     \n",
      "‚îÇ‚îÇ‚îÇ     ‚îå‚îÄ‚îÄ‚ñ∫ ,         punct    ,         PUNCT     \n",
      "‚îÇ‚îÇ‚îÇ     ‚îÇ‚îå‚îÄ‚ñ∫ on        nsubj    on        PRON      \n",
      "‚îî‚î¥‚î¥‚îÄ‚îÄ‚î¨‚î¨‚îÄ‚î¥‚î¥‚îÄ‚îÄ vendait   ROOT     vendre    VERB      \n",
      "     ‚îÇ‚îÇ  ‚îå‚îÄ‚ñ∫ des       det      un        DET       \n",
      "     ‚îÇ‚îî‚îÄ‚ñ∫‚îî‚îÄ‚îÄ fruits    obj      fruit     NOUN      \n",
      "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ .         punct    .         PUNCT     \n"
     ]
    }
   ],
   "source": [
    "import explacy\n",
    "explacy.print_parse_info(nlp, 'Derri√®re lui, sur le carreau de la rue Rambuteau, on vendait des fruits.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi r√©cup√©rer parcourir les tokens et afficher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derri√®re CASE lui\n",
      "lui OBL:MOD vendait\n",
      ", PUNCT vendait\n",
      "sur CASE carreau\n",
      "le DET carreau\n",
      "carreau OBL:MOD vendait\n",
      "de CASE rue\n",
      "la DET rue\n",
      "rue NMOD carreau\n",
      "Rambuteau NMOD rue\n",
      ", PUNCT vendait\n",
      "on NSUBJ vendait\n",
      "vendait ROOT vendait\n",
      "des DET fruits\n",
      "fruits OBJ vendait\n",
      ". PUNCT vendait\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.dep_.upper(), token.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les attributs de token suivant peuvent √™tre utilis√©s pour parcourir l'arbre de d√©pendance :¬†\n",
    "- `children` les tokens d√©pendants du token\n",
    "- `subtree` tous les descendants du token\n",
    "- `ancestors` tous les parents du token\n",
    "- `rights` les enfants √† droite du token\n",
    "- `lefts` les enfants √† gauche du token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut extraire de la phrase pr√©c√©dente le triplet sujet-verbe-objet comme ceci :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(on, vendait, fruits)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = [token for token in doc if token.head == token][0]\n",
    "subjects = [tok for tok in root.lefts if tok.dep_ == \"nsubj\"]\n",
    "subject = subjects[0]\n",
    "objs = [tok for tok in root.rights if tok.dep_ == \"obj\"]\n",
    "obj = objs[0]\n",
    "subject, root, obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "des\n",
      "fruits\n"
     ]
    }
   ],
   "source": [
    "for obj in objs:\n",
    "    for descendant in obj.subtree:\n",
    "        print(descendant.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä¬†vous\n",
    "\n",
    "1. Trouver et afficher l'objet de la phrase :¬†¬´ Depuis que Google a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle. ¬ª\n",
    "\n",
    "2. Dans la liste `texts` vue plus haut, retrouvez les verbes dont Cadine ou le pronom 'elle' est sujet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dep tree                  Token      Dep type   Lemma      Part of Sp\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Depuis     mark       depuis     ADP       \n",
      "                   ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚ñ∫ que        mark       que        SCONJ     \n",
      "                   ‚îÇ‚îÇ‚îå‚îÄ‚îÄ‚ñ∫ Google     nsubj      Google     PROPN     \n",
      "                   ‚îÇ‚îÇ‚îÇ‚îå‚îÄ‚ñ∫ a          aux:tense  avoir      AUX       \n",
      "‚îå‚îÄ‚ñ∫‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚î¥‚î¥‚î¥‚îÄ‚îÄ annonc√©    advcl      annoncer   VERB      \n",
      "‚îÇ  ‚îÇ     ‚îÇ            ‚îå‚îÄ‚ñ∫ son        det        son        DET       \n",
      "‚îÇ  ‚îÇ     ‚îî‚îÄ‚ñ∫‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ intention  obj        intention  NOUN      \n",
      "‚îÇ  ‚îÇ        ‚îÇ         ‚îå‚îÄ‚ñ∫ de         mark       de         ADP       \n",
      "‚îÇ  ‚îÇ        ‚îî‚îÄ‚ñ∫‚îå‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ stopper    acl        stopper    VERB      \n",
      "‚îÇ  ‚îÇ           ‚îÇ‚îÇ  ‚îå‚îÄ‚ñ∫‚îå‚îÄ‚îÄ d'         case       de         ADP       \n",
      "‚îÇ  ‚îÇ           ‚îÇ‚îÇ  ‚îÇ  ‚îî‚îÄ‚ñ∫ ici        fixed      ici        ADV       \n",
      "‚îÇ  ‚îÇ           ‚îÇ‚îÇ  ‚îÇ  ‚îå‚îÄ‚ñ∫ deux       nummod     deux       NUM       \n",
      "‚îÇ  ‚îÇ           ‚îÇ‚îî‚îÄ‚ñ∫‚îî‚îÄ‚îÄ‚î¥‚îÄ‚îÄ ans        obl:mod    an         NOUN      \n",
      "‚îÇ  ‚îÇ           ‚îÇ      ‚îå‚îÄ‚ñ∫ les        det        le         DET       \n",
      "‚îÇ  ‚îÇ           ‚îî‚îÄ‚îÄ‚ñ∫‚îå‚îÄ‚îÄ‚îº‚îÄ‚îÄ cookies    obj        cookie     NOUN      \n",
      "‚îÇ  ‚îÇ               ‚îÇ  ‚îî‚îÄ‚ñ∫ tiers      amod       tiers      ADJ       \n",
      "‚îÇ  ‚îÇ               ‚îÇ  ‚îå‚îÄ‚ñ∫ sur        case       sur        ADP       \n",
      "‚îÇ  ‚îÇ               ‚îî‚îÄ‚ñ∫‚îî‚îÄ‚îÄ Chrome     nmod       Chrome     PROPN     \n",
      "‚îÇ  ‚îÇ                 ‚îå‚îÄ‚îÄ‚ñ∫ ,          punct      ,          PUNCT     \n",
      "‚îÇ  ‚îÇ                 ‚îÇ‚îå‚îÄ‚ñ∫ son        det        son        DET       \n",
      "‚îÇ  ‚îî‚îÄ‚ñ∫‚îå‚î¨‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚î¥‚î¥‚îÄ‚îÄ moteur     obl:mod    moteur     NOUN      \n",
      "‚îÇ     ‚îÇ‚îÇ‚îÇ          ‚îÇ  ‚îå‚îÄ‚ñ∫ de         case       de         ADP       \n",
      "‚îÇ     ‚îÇ‚îÇ‚îÇ          ‚îî‚îÄ‚ñ∫‚îî‚îÄ‚îÄ recherche  nmod       recherche  NOUN      \n",
      "‚îÇ     ‚îÇ‚îÇ‚îÇ            ‚îå‚îÄ‚îÄ‚ñ∫ qui        nsubj:pass qui        PRON      \n",
      "‚îÇ     ‚îÇ‚îÇ‚îÇ            ‚îÇ‚îå‚îÄ‚ñ∫ est        aux:pass   √™tre       AUX       \n",
      "‚îÇ     ‚îÇ‚îÇ‚îî‚îÄ‚ñ∫‚îå‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚î¥‚îÄ‚îÄ utilis√©    acl:relcl  utiliser   VERB      \n",
      "‚îÇ     ‚îÇ‚îÇ   ‚îÇ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ par        case       par        ADP       \n",
      "‚îÇ     ‚îÇ‚îÇ   ‚îÇ‚îÇ  ‚îÇ   ‚îå‚îÄ‚ñ∫‚îå‚îÄ‚îÄ plus       advmod     plus       ADV       \n",
      "‚îÇ     ‚îÇ‚îÇ   ‚îÇ‚îÇ  ‚îÇ   ‚îÇ  ‚îî‚îÄ‚ñ∫ de         fixed      de         ADP       \n",
      "‚îÇ     ‚îÇ‚îÇ   ‚îÇ‚îÇ  ‚îÇ‚îå‚îÄ‚ñ∫‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 60         nummod     60         NUM       \n",
      "‚îÇ     ‚îÇ‚îÇ   ‚îÇ‚îî‚îÄ‚ñ∫‚îî‚î¥‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ %          obl:mod    pourcent   NOUN      \n",
      "‚îÇ     ‚îÇ‚îÇ   ‚îÇ      ‚îÇ  ‚îå‚îÄ‚îÄ‚ñ∫ de         case       de         ADP       \n",
      "‚îÇ     ‚îÇ‚îÇ   ‚îÇ      ‚îÇ  ‚îÇ‚îå‚îÄ‚ñ∫ la         det        le         DET       \n",
      "‚îÇ     ‚îÇ‚îÇ   ‚îÇ      ‚îî‚îÄ‚ñ∫‚îú‚îº‚îÄ‚îÄ population nmod       population NOUN      \n",
      "‚îÇ     ‚îÇ‚îÇ   ‚îÇ         ‚îÇ‚îî‚îÄ‚ñ∫ mondiale   amod       mondial    ADJ       \n",
      "‚îÇ     ‚îÇ‚îÇ   ‚îÇ         ‚îî‚îÄ‚îÄ‚ñ∫ connect√©e  amod       connecter  VERB      \n",
      "‚îÇ     ‚îÇ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ,          punct      ,          PUNCT     \n",
      "‚îÇ     ‚îÇ‚îÇ              ‚îå‚îÄ‚ñ∫ les        det        le         DET       \n",
      "‚îÇ     ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ Criteo     appos      criteo     NOUN      \n",
      "‚îÇ     ‚îÇ            ‚îÇ  ‚îå‚îÄ‚ñ∫ ,          punct      ,          PUNCT     \n",
      "‚îÇ     ‚îÇ            ‚îî‚îÄ‚ñ∫‚îî‚îÄ‚îÄ LiveRamp   conj       LiveRamp   PROPN     \n",
      "‚îÇ     ‚îÇ               ‚îå‚îÄ‚ñ∫ et         cc         et         CCONJ     \n",
      "‚îÇ     ‚îÇ            ‚îå‚îÄ‚ñ∫‚îî‚îÄ‚îÄ autres     amod       autre      ADJ       \n",
      "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ Index      nmod       Index      X         \n",
      "‚îÇ                     ‚îî‚îÄ‚ñ∫ Exchange   flat:name  Exchange   X         \n",
      "‚îÇ                     ‚îå‚îÄ‚ñ∫ se         expl:comp  se         PRON      \n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ pr√©parent  ROOT       pr√©parer   VERB      \n",
      "      ‚îÇ‚îî‚îÄ‚ñ∫‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ √†          advmod     √†          ADP       \n",
      "      ‚îÇ   ‚îÇ           ‚îî‚îÄ‚ñ∫ ce         fixed      ce         PRON      \n",
      "      ‚îÇ   ‚îÇ           ‚îå‚îÄ‚ñ∫ qui        nsubj      qui        PRON      \n",
      "      ‚îÇ   ‚îî‚îÄ‚ñ∫‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ peut       acl:relcl  pouvoir    VERB      \n",
      "      ‚îÇ      ‚îÇ        ‚îå‚îÄ‚ñ∫ √™tre       aux:pass   √™tre       AUX       \n",
      "      ‚îÇ      ‚îî‚îÄ‚ñ∫‚îå‚î¨‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ consid√©r√©  xcomp      consid√©rer VERB      \n",
      "      ‚îÇ         ‚îÇ‚îÇ‚îÇ  ‚îå‚îÄ‚îÄ‚ñ∫ comme      case       comme      ADP       \n",
      "      ‚îÇ         ‚îÇ‚îÇ‚îÇ  ‚îÇ‚îå‚îÄ‚ñ∫ un         det        un         DET       \n",
      "      ‚îÇ         ‚îÇ‚îÇ‚îî‚îÄ‚ñ∫‚îî‚î¥‚îÄ‚îÄ s√©isme     obl:mod    s√©isme     NOUN      \n",
      "      ‚îÇ         ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ,          punct      ,          PUNCT     \n",
      "      ‚îÇ         ‚îÇ    ‚îå‚îÄ‚îÄ‚ñ∫ √†          case       √†          ADP       \n",
      "      ‚îÇ         ‚îÇ    ‚îÇ‚îå‚îÄ‚ñ∫ leur       det        leur       DET       \n",
      "      ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚ñ∫‚îî‚î¥‚îÄ‚îÄ √©chelle    obl:mod    √©chelle    NOUN      \n",
      "      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ .          punct      .          PUNCT     \n"
     ]
    }
   ],
   "source": [
    "import explacy\n",
    "explacy.print_parse_info(nlp, \"Depuis que Google a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Depuis que Google a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle.\")\n",
    "root = [token for token in doc if token.head == token][0]\n",
    "objs = [tok for tok in root.rights if tok.dep_ ==\"obj\"]\n",
    "for obj in objs:\n",
    "    for descendant in obj.subtree:\n",
    "        print(descendant.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matching par r√®gle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy a une classe `Matcher` qui permet de rep√©rer des tokens ou des suites de tokens √† l'aide de patrons (*pattern*). Ces patrons peuvent porter sur la forme des tokens ou leurs attributs (pos, ent).  \n",
    "On peut aussi utiliser des cat√©gories comme `IS_ALPHA` ou `IS_NUM`, voir la [doc](https://spacy.io/usage/rule-based-matching#adding-patterns-attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 8 en taille M\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LOWER\": \"en\"}, {\"LOWER\": \"taille\"}, {\"IS_ALPHA\": True, \"IS_UPPER\": True}]\n",
    "# en taille + lettres en maj\n",
    "matcher.add(\"tailles\", [pattern])\n",
    "\n",
    "doc = nlp(\"Ce mod√®le est aussi disponible en taille M ; je vous le conseille.\")\n",
    "matches = matcher(doc)\n",
    "for _, start, end in matches:\n",
    "    #string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√áa fonctionne pour les s√©quences comme ¬´ en taille M ¬ª ou ¬´ en taille XL ¬ª mais pas pour ¬´ vous l'avez en XL ? ¬ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"vous l'avez en XL ?\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut essayer d'am√©liorer les r√®gles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en XL\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_1 = [{\"LOWER\": \"en\"}, {\"LOWER\": \"taille\"}, {\"IS_ALPHA\": True, \"IS_UPPER\": True}]\n",
    "pattern_2 = [{\"LOWER\": \"en\"}, {\"IS_ALPHA\": True, \"IS_UPPER\": True}]\n",
    "matcher.add(\"tailles\", [pattern_1, pattern_2])\n",
    "# r√®gle avec deux patterns\n",
    "\n",
    "doc = nlp(\"vous l'avez en XL ?\")\n",
    "matches = matcher(doc)\n",
    "for _, start, end in matches:\n",
    "    #string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou encore :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9364735015326875510 tailles 3 5 en XL\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "sizes = ['XS', 'S', 'M', 'L', 'XL']\n",
    "pattern_1 = [{\"LOWER\": \"en\"}, {\"LOWER\": \"taille\"}, {\"TEXT\": {\"IN\": sizes}}]\n",
    "pattern_2 = [{\"LOWER\": \"en\"}, {\"TEXT\": {\"IN\": sizes}}]\n",
    "matcher.add(\"tailles\", [pattern_1, pattern_2])\n",
    "# r√®gle avec deux patterns\n",
    "\n",
    "doc = nlp(\"vous l'avez en XL ?\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä vous\n",
    "\n",
    "Dans `data/Le_Ventre_de_Paris-short.txt`, trouver les s√©quences pronom - le lemme 'vendre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PRON\"}, {\"LEMMA\": \"vendre\"}]\n",
    "matcher.add(\"pro_vendre\", [pattern])\n",
    "\n",
    "doc = \"\"\n",
    "with open('data/Le_Ventre_de_Paris-short.txt') as input_data:\n",
    "    doc = nlp(input_data.read())\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, start, end in matches:\n",
    "    #string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dependancy Matcher :¬†extraction de patrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depuis la v3, Spacy a ajout√© un *Dependancy Matcher* qui permet de faire de l'extraction de patrons syntaxiques. Il est maintenant possible de faire porter des requ√™tes sur l'arbre syntaxique et non plus seulement sur la s√©quence des tokens.  \n",
    "Ce dispositif utilise [Semgrex](https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.html), la syntaxe utilis√©e dans Tgrep et Tregex, les outils de requ√™te sur Treebank de Stanford.\n",
    "\n",
    "Voir la [documentation](https://spacy.io/usage/rule-based-matching#dependencymatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ventre_short = \"\"\n",
    "with open('data/Le_Ventre_de_Paris-short.txt') as input_f:\n",
    "    ventre_short = input_f.read()\n",
    "doc = nlp(ventre_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "pattern = [\n",
    "  {\n",
    "    \"RIGHT_ID\": \"vendre\",    \n",
    "    \"RIGHT_ATTRS\": {\"LEMMA\": \"vendre\"}\n",
    "  }\n",
    "]\n",
    "matcher.add(\"VENDRE\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for m_id, t_ids in matches:\n",
    "    for t_id in t_ids:\n",
    "        print(doc[t_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "pattern = [\n",
    "    {\n",
    "        \"RIGHT_ID\": \"vendre\",    \n",
    "        \"RIGHT_ATTRS\": {\"LEMMA\": {\"IN\": [\"vendre\", \"acheter\"]}}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"vendre\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"sujet\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"},  \n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"vendre\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"objet\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"obj\", \"iobj\", \"obl\"]}},  \n",
    "    }\n",
    "]\n",
    "matcher.add(\"VENDRE\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for m_id, t_ids in matches:\n",
    "    print(\"verbe, sujet, objet :¬†\", \" -> \".join([doc[t_id].text for t_id in t_ids]))\n",
    "    print(\"objet complet :¬†\", \" \".join([t.text for t in doc[t_ids[2]].subtree]))\n",
    "    print(\"Phrase compl√©te :¬†\", doc[t_ids[0]].sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä vous\n",
    "\n",
    "Ajouter une r√®gle au motif pour trouver aussi l'objet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapter les traitements de Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. re-tokenisation\n",
    "\n",
    "- voir [https://spacy.io/usage/linguistic-features#retokenization](https://spacy.io/usage/linguistic-features#retokenization)\n",
    "\n",
    "Dans l'exemple qui suit ¬´ quer-cra ¬ª sera tokeniz√© √† tort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Pour les bons bails √ßa va grave quer-cra\")\n",
    "print([(tok.text, tok.pos_, tok.lemma_)for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc[7:], attrs={\"LEMMA\": \"quer-cra\", \"POS\": \"NOUN\"})\n",
    "print([(tok.text, tok.pos_) for tok in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention ici c‚Äôest l‚Äôobjet doc qui est modifi√©, le r√©sultat mais pas le traitement. Nous allons voir comment faire pour modifier le traitement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modification de la tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH, LEMMA, POS, TAG\n",
    "\n",
    "special_case = [{ORTH: \"quer-cra\"}]\n",
    "nlp.tokenizer.add_special_case(\"quer-cra\", special_case)\n",
    "doc = nlp(\"Pour les bons bails √ßa va grave quer-cra\")\n",
    "print([(tok.text, tok.pos_, tok.lemma_) for tok in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien modifi√© la tokenisation dans le mod√®le `nlp`. Cela n'affecte pas par contre l'√©tiquetage en POS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entit√©s nomm√©es :¬†traitement par r√®gles\n",
    " - Voir [https://spacy.io/usage/rule-based-matching#entityruler](https://spacy.io/usage/rule-based-matching#entityruler)\n",
    " \n",
    "Spacy offre aussi un m√©canisme de traitement par r√®gle pour les entit√©s nomm√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "doc = nlp(\"Depuis que machin a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle.\")\n",
    "print(\"Avant : \", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", config={'overwrite_ents':True})\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Chrome\"},\n",
    "            {\"label\":\"ORG\", \"pattern\":\"machin\"},\n",
    "    {\"label\":\"ORG\", \"pattern\":\"Criteo\"},\n",
    "    {\"label\":\"ORG\",\"pattern\":\"LiveRamp\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"Depuis que machin a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle.\")\n",
    "print(\"Apr√®s : \", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
